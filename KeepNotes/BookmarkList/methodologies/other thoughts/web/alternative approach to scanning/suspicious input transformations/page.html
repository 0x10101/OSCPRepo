<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Suspicious Input Transformations</title>
</head><body>Suspicious input transformations<br/>
The initial probe used to identify suspicious behaviour should be as simple and generic as possible. Take the following payload which exploits FreeMarker SSTI:<br/>
&lt;#assign ex="freemarker.template.utility.Execute"?new()&gt; ${ex("id")}<br/>
<br/>
We can easily roll this back to a more generic payload that will identify most template engines using a specific popular statement syntax:<br/>
${7*7} (expect 49)<br/>
<br/>
Can we expand the coverage of this to detect generic code evaluation? We could try something like:<br/>
7*7 (expect 49)<br/>
<br/>
but that will only work on numeric inputs. To detect injection into strings, we need something like:<br/>
\x41 (expect A)<br/>
<br/>
However many languages, notably including SQL, don't support hex escapes. This probe can be made one step more generic, to support almost every language:<br/>
\\ (expect \)<br/>
<br/>
At this point we have our very first probe for detecting suspicious input transformations. We can now move to the 'Scan' stage of the development process, trying out this payload on a range of applications and seeing what it throws up. Provided the probe is good and the testbed is large enough (more on that later), we'll get a suitably sized set of results which we can manually investigate to find out what's interesting.<br/>
<br/>
In this case, the first step to understanding the behaviour is to look for other input transformations like \x41=&gt;A. By comparing the application's handling of a known-bad escape sequence with other characters, we can gain subtle clues to which characters has special significance server-side. For example, using the baseline of \zz we can easily spot the anomaly:<br/>
\zz =&gt; \zz<br/>
<br/>
\" =&gt; \"<br/>
\$ =&gt; \$<br/>
<span style="color: #ff0000">\{ =&gt; {</span><br/>
\x41 =&gt; \x41<br/>
<br/>
This tells us that the { character has special significance. Having repeated and refined this manual investigation process a few times, we can loop back around to the 'Implement' stage and automate it. Here's a screenshot of the scanner's output on a page that is vulnerable to Markdown injection:<br/>
<img src="image.png" /><br/>
<br/>
<br/>
And a page that isn't vulnerable to anything, but merely calls stripslashes() on the input:<br/>
<img src="image 2.png" /><br/>
<br/>
<br/>
This automated followup means that we can tell how exploitable the endpoint is at a glance. A potential further refinement would be to recognise and classify specific transformation fingerprints.<br/>
<br/>
Note that even though this technique is capable of detecting a huge range of vulnerabilities, on most inputs it only sends a single request. This combination of flexibility and efficiency is at the heart of iterative scanning.<br/>
<br/>
If you're aware of (or able to construct) targets that are definitely vulnerable, you can verify the scanner's susceptibility to false negatives. I found the scanner failed to identify vulnerabilities in JSON responses, since although the server would decode \\ to \, it would then escape the \ back to \\ when embedding it in a JSON string. This was easily fixed by JSON decoding responses where appropriate.<br/>
<br/>
Unfortunately, there's a more serious weakness. This approach relies on user input being reflected after it's been processed. For example, if an application places user input into a SQL SELECT statement, but never displays that query, the vulnerability will be missed entirely. This is a fundamental flaw with relying on suspicious input transformations to detect vulnerabilities.</body></html>
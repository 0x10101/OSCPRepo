<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Metadata Leakage</title>
</head><body><b>Summary</b><br/>
This section describes how to test the robots.txt file for information leakage &nbsp;of &nbsp;the &nbsp;web &nbsp;application’s &nbsp;directory &nbsp;or &nbsp;folder &nbsp;path(s). &nbsp;Fur-thermore, &nbsp;the &nbsp;list &nbsp;of &nbsp;directories &nbsp;that &nbsp;are &nbsp;to &nbsp;be &nbsp;avoided &nbsp;by &nbsp;Spiders, &nbsp;Robots, &nbsp;or &nbsp;Crawlers &nbsp;can &nbsp;also &nbsp;be &nbsp;created &nbsp;as &nbsp;a &nbsp;dependency &nbsp;for &nbsp;Map &nbsp;execution paths through application (OTG-INFO-007)<br/>
<br/>
<b>Test Objectives</b><br/>
1. Information &nbsp;leakage &nbsp;of &nbsp;the &nbsp;web &nbsp;application’s &nbsp;directory &nbsp;or &nbsp;folder &nbsp;path(s).<br/>
2. Create the list of directories that are to be avoided by Spiders, Ro-bots, or Crawlers.<br/>
<br/>
<b>How to Test</b><br/>
robots.txt<br/>
<br/>
Web &nbsp;Spiders, &nbsp;Robots, &nbsp;or &nbsp;Crawlers &nbsp;retrieve &nbsp;a &nbsp;web &nbsp;page &nbsp;and &nbsp;then &nbsp;re-cursively &nbsp;traverse &nbsp;hyperlinks &nbsp;to &nbsp;retrieve &nbsp;further &nbsp;web &nbsp;content. &nbsp;Their &nbsp;accepted &nbsp;behavior &nbsp;is &nbsp;specified &nbsp;by &nbsp;the &nbsp;Robots &nbsp;Exclusion &nbsp;Protocol &nbsp;of &nbsp;the robots.txt file in the web root directory [1].<br/>
<br/>
As an example, the beginning of the robots.txt file from http://www.google.com/robots.txt sampled on 11 August 2013 is quoted below:<br/>
<br/>
User-agent: *<br/>
Disallow: /search<br/>
Disallow: /sdch<br/>
Disallow: /groups<br/>
Disallow: /images<br/>
Disallow: /catalogs<br/>
...<br/>
<br/>
The &nbsp;User-Agent &nbsp;directive &nbsp;refers &nbsp;to &nbsp;the &nbsp;specific &nbsp;web &nbsp;spider/robot/crawler. For example the User-Agent: Googlebot refers to the spider from &nbsp;Google &nbsp;while &nbsp;“User-Agent: &nbsp;bingbot”[1] &nbsp;refers &nbsp;to &nbsp;crawler &nbsp;from &nbsp;Microsoft/Yahoo!. &nbsp;User-Agent: &nbsp;* &nbsp;in &nbsp;the &nbsp;example &nbsp;above &nbsp;applies &nbsp;to &nbsp;all &nbsp;web spiders/robots/crawlers [2] as quoted below: <br/>
User-agent: *<br/>
<br/>
The &nbsp;Disallow &nbsp;directive &nbsp;specifies &nbsp;which &nbsp;resources &nbsp;are &nbsp;prohibited &nbsp;by &nbsp;spiders/robots/crawlers. &nbsp;In &nbsp;the &nbsp;example &nbsp;above, &nbsp;directories &nbsp;such &nbsp;as &nbsp;the following are prohibited:<br/>
... <br/>
Disallow: /search<br/>
Disallow: /sdch<br/>
Disallow: /groups<br/>
Disallow: /images<br/>
Disallow: /catalogs<br/>
...<br/>
<br/>
Web spiders/robots/crawlers can intentionally ignore the Disallow directives &nbsp;specified &nbsp;in &nbsp;a &nbsp;robots.txt &nbsp;file &nbsp;[3], &nbsp;such &nbsp;as &nbsp;those &nbsp;from &nbsp;Social Networks[2] to ensure that shared linked are still valid. Hence, robots.txt should not be considered as a mechanism to enforce re-strictions &nbsp;on &nbsp;how &nbsp;web &nbsp;content &nbsp;is &nbsp;accessed, &nbsp;stored, &nbsp;or &nbsp;republished &nbsp;by third parties.<br/>
robots.txt in webroot - with “wget” or “curl”<br/>
<br/>
The robots.txt file is retrieved from the web root directory of the web server. For example, to retrieve the robots.txt from www.google.com using “wget” or “curl”:<br/>
<br/>
cmlh$ wget http://www.google.com/robots.txt<br/>
--2013-08-11 14:40:36-- &nbsp;http://www.google.com/robots.txt<br/>
Resolving &nbsp;www.google.com... &nbsp;74.125.237.17, &nbsp;74.125.237.18, &nbsp;74.125.237.19, ...<br/>
Connecting &nbsp;to &nbsp;www.google.com|74.125.237.17|:80... &nbsp;connected.<br/>
HTTP request sent, awaiting response... 200 OK<br/>
Length: unspecified [text/plain]<br/>
Saving to: ‘robots.txt.1’ &nbsp; &nbsp;<br/>
[ &lt;=&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ] 7,074 &nbsp; &nbsp; &nbsp; --.-K/s &nbsp; in 0s &nbsp; &nbsp; &nbsp;2013-08-11 14:40:37 (59.7 MB/s) - ‘robots.txt’ saved [7074]<br/>
<br/>
cmlh$ head -n5 robots.txt<br/>
User-agent: *<br/>
Disallow: /search<br/>
Disallow: /sdch<br/>
Disallow: /groups<br/>
Disallow: /images<br/>
<br/>
<br/>
cmlh$ curl -O http://www.google.com/robots.txt &nbsp; &nbsp;<br/>
% &nbsp;Total &nbsp; &nbsp; &nbsp; &nbsp;% &nbsp;Received &nbsp;% &nbsp;Xferd &nbsp; &nbsp;Average &nbsp;Speed &nbsp; &nbsp; &nbsp;Time &nbsp; &nbsp; &nbsp; &nbsp;Time &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Time &nbsp;Current &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Dload &nbsp;Upload &nbsp; Total &nbsp; Spent &nbsp; &nbsp;Left &nbsp;Speed101 &nbsp;7074 &nbsp; &nbsp;0 &nbsp;7074 &nbsp; &nbsp;0 &nbsp; &nbsp; 0 &nbsp; 9410 &nbsp; &nbsp; &nbsp;0 --:--:-- --:--:-- --:--:-- 27312<br/>
<br/>
cmlh$ head -n5 robots.txt<br/>
User-agent: *<br/>
Disallow: /search<br/>
Disallow: /sdch<br/>
Disallow: /groups<br/>
Disallow: /images<br/>
<br/>
<br/>
<b>robots.txt in webroot - with rockspider</b><br/>
“rockspider”[3] automates the creation of the initial scope for Spiders/Robots/Crawlers of files and directories/folders of a web site.<br/>
<br/>
For example, to create the initial scope based on the Allowed: directive from www.google.com using “rockspider”[4]:<br/>
cmlh$ ./rockspider.pl -www www.google.com<br/>
<br/>
“Rockspider” Alpha v0.1_2<br/>
<br/>
Copyright 2013 Christian Heinrich<br/>
Licensed under the Apache License, Version 2.0<br/>
1. Downloading http://www.google.com/robots.txt<br/>
2. “robots.txt” saved as “www.google.com-robots.txt”<br/>
3. &nbsp;Sending &nbsp;Allow: &nbsp;URIs &nbsp;of &nbsp;www.google.com &nbsp;to &nbsp;web &nbsp;proxy &nbsp;i.e. &nbsp;127.0.0.1:8080 <br/>
&nbsp;&#09;/catalogs/about sent <br/>
&nbsp;&#09;/catalogs/p? sent <br/>
&nbsp;&#09;/news/directory sent<br/>
&nbsp;&#09;...<br/>
4. Done.<br/>
<br/>
<b>Analyze robots.txt using Google Webmaster Tools</b><br/>
Web site owners can use the Google “Analyze robots.txt” function to analyse the website as part of its “Google Webmaster Tools” (https://www.google.com/webmasters/tools). &nbsp;This &nbsp;tool &nbsp;can &nbsp;assist &nbsp;with &nbsp;test-ing and the procedure is as follows:<br/>
1. Sign into Google Webmaster Tools with a Google account.<br/>
2. On the dashboard, write the URL for the site to be analyzed.<br/>
3. Choose &nbsp;between &nbsp;the &nbsp;available &nbsp;methods &nbsp;and &nbsp;follow &nbsp;the &nbsp;on &nbsp;screen &nbsp;instruction.<br/>
<br/>
<b>META Tag</b><br/>
&lt;META&gt; tags are located within the HEAD section of each HTML Doc-ument and should be consistent across a web site in the likely event that the robot/spider/crawler start point does not begin from a docu-ment link other than webroot i.e. a “deep link”[5].<br/>
<br/>
If there is no “&lt;META NAME=”ROBOTS” ... &gt;” entry then the “Robots Exclusion Protocol” defaults to “INDEX,FOLLOW” respectively. Therefore, the other two valid entries defined by the “Robots Exclusion Pro-tocol” are prefixed with “NO...” i.e. “NOINDEX” and “NOFOLLOW”.<br/>
<br/>
Web &nbsp;spiders/robots/crawlers &nbsp;can &nbsp;intentionally &nbsp;ignore &nbsp;the &nbsp;“&lt;META &nbsp;NAME=”ROBOTS”” &nbsp;tag &nbsp;as &nbsp;the &nbsp;robots.txt &nbsp;file &nbsp;convention &nbsp;is &nbsp;preferred. &nbsp;Hence, <u>&lt;META&gt; Tags should not be considered the primary mechanism, rather a complementary control to robots.txt</u>.<br/>
<br/>
<b>&lt;META&gt; Tags - with Burp</b><br/>
Based &nbsp;on &nbsp;the &nbsp;Disallow &nbsp;directive(s) &nbsp;listed &nbsp;within &nbsp;the &nbsp;robots.txt &nbsp;file &nbsp;in &nbsp;webroot, a regular expression search for “&lt;META NAME=”ROBOTS”” within &nbsp;each &nbsp;web &nbsp;page &nbsp;is &nbsp;undertaken &nbsp;and &nbsp;the &nbsp;result &nbsp;compared &nbsp;to &nbsp;the &nbsp;robots.txt file in webroot.For &nbsp;example, &nbsp;the &nbsp;robots.txt &nbsp;file &nbsp;from &nbsp;facebook.com &nbsp;has &nbsp;a &nbsp;“Disallow: &nbsp;/ac.php” &nbsp;entry[6] &nbsp;and &nbsp;the &nbsp;resulting &nbsp;search &nbsp;for &nbsp;“&lt;META &nbsp;NAME=”ROBOTS””.<br/>
<img src="image.png" /><br/>
<br/>
The &nbsp;above &nbsp;might &nbsp;be &nbsp;considered &nbsp;a &nbsp;fail &nbsp;since &nbsp;“INDEX,FOLLOW” &nbsp;is &nbsp;the &nbsp;default &lt;META&gt; Tag specified by the “Robots Exclusion Protocol” yet “Disallow: /ac.php” is listed in robots.txt.<br/>
<br/>
<b>Tools</b><br/>
Browser (View Source function)<br/>
curl<br/>
wget<br/>
rockspider[7]<br/>
<br/>
<b>References<br/>
</b><b>Whitepapers</b><br/>
[1] “The Web Robots Pages” - http://www.robotstxt.org/<br/>
[2] “Block and Remove Pages Using a robots.txt File” - https://support.google.com/webmasters/answer/156449<br/>
[3] &nbsp;“(ISC)2 &nbsp;Blog: &nbsp;The &nbsp;Attack &nbsp;of &nbsp;the &nbsp;Spiders &nbsp;from &nbsp;the &nbsp;Clouds” &nbsp;- &nbsp;http://blog.isc2.org/isc2_blog/2008/07/the-attack-of-t.html<br/>
[4] &nbsp; &nbsp;“Telstra &nbsp; &nbsp;customer &nbsp; &nbsp;database &nbsp; &nbsp;exposed” &nbsp; &nbsp;- &nbsp; &nbsp;http://www.smh.com.au/it-pro/security-it/telstra-customer-database-ex-posed-20111209-1on60.html</body></html>